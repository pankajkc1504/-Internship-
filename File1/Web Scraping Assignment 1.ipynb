{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1b4a2d8-2583-4d29-a846-0a06e080e694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Rating, Year of Release]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = 'https://www.imdb.com/list/ls056092300/'\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "movies = soup.find_all('div', class_='lister-item mode-detail')\n",
    "\n",
    "movie_names = []\n",
    "movie_ratings = []\n",
    "movie_years = []\n",
    "\n",
    "for movie in movies:\n",
    "    name = movie.h3.a.text\n",
    "    movie_names.append(name)\n",
    "    \n",
    "    rating = movie.find('span', class_='ipl-rating-star__rating').text\n",
    "    movie_ratings.append(float(rating))\n",
    "    \n",
    "    year = movie.find('span', class_='lister-item-year text-muted unbold').text\n",
    "    year = year.strip('()').split()[0]  # Clean the year\n",
    "    movie_years.append(int(year))\n",
    "\n",
    "# DataFrame\n",
    "movies_df = pd.DataFrame({'Name': movie_names,'Rating': movie_ratings,'Year of Release': movie_years})\n",
    "\n",
    "print(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e05d4a0-6cfc-46ca-b287-311eb97f4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Heading, Date, Content, Video Likes]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = 'https://www.patreon.com/coreyms'\n",
    "\n",
    "# Request to fetch the content of the webpage\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    headings = []\n",
    "    dates = []\n",
    "    contents = []\n",
    "    video_likes = []\n",
    "    \n",
    "    posts = soup.find_all('div', class_='sc-16b9v2b-0')\n",
    "    \n",
    "    for post in posts:\n",
    "        # Extract heading\n",
    "        heading_tag = post.find('h2')\n",
    "        heading = heading_tag.text.strip() if heading_tag else \"No Heading\"\n",
    "        headings.append(heading)\n",
    "        \n",
    "        # Extract date\n",
    "        date_tag = post.find('time')\n",
    "        date = date_tag.text.strip() if date_tag else \"No Date\"\n",
    "        dates.append(date)\n",
    "        \n",
    "        # Extract content\n",
    "        content_tag = post.find('div', class_='sc-12p78pa-0')\n",
    "        content = content_tag.text.strip() if content_tag else \"No Content\"\n",
    "        contents.append(content)\n",
    "        \n",
    "        # Extract YouTube link\n",
    "        video_tag = post.find('a', href=True)\n",
    "        video_link = video_tag['href'] if video_tag and 'youtube' in video_tag['href'] else \"No Video Link\"\n",
    "        if video_link != \"No Video Link\":\n",
    "            video_likes.append(\"Likes Not Scraped\")\n",
    "        else:\n",
    "            video_likes.append(\"No Likes\")\n",
    "    \n",
    "# DataFrame\n",
    "    posts_df = pd.DataFrame({'Heading': headings,'Date': dates,'Content': contents,'Video Likes': video_likes})\n",
    "    \n",
    "    print(posts_df)\n",
    "else:\n",
    "    print(\"Failed to retrieve the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0be0372-4607-4c0c-a8a1-f8008dc93b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title, Location, Area, Price, EMI]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Define base URL and headers\n",
    "base_url = \"https://www.nobroker.in/property/sale/\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36'}\n",
    "\n",
    "# List of localities\n",
    "localities = ['Indira-Nagar', 'Jayanagar', 'Rajaji-Nagar']\n",
    "\n",
    "# Lists to store scraped data\n",
    "titles = []\n",
    "locations = []\n",
    "areas = []\n",
    "emis = []\n",
    "prices = []\n",
    "\n",
    "# Function to scrape data for a given locality\n",
    "def scrape_locality_data(locality):\n",
    "    url = base_url + locality.replace(' ', '-')\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract the details of houses\n",
    "        property_listings = soup.find_all('div', class_='card__details-container')\n",
    "\n",
    "        for property in property_listings:\n",
    "            # Extract the house title\n",
    "            title_tag = property.find('span', class_='card__title')\n",
    "            title = title_tag.text if title_tag else 'No Title'\n",
    "            \n",
    "            # Extract location\n",
    "            location_tag = property.find('div', class_='card__locality')\n",
    "            location = location_tag.text if location_tag else 'No Location'\n",
    "            \n",
    "            # Extract area\n",
    "            area_tag = property.find('div', class_='card__area')\n",
    "            area = area_tag.text if area_tag else 'No Area'\n",
    "            \n",
    "            # Extract price\n",
    "            price_tag = property.find('div', class_='card__price')\n",
    "            price = price_tag.text.strip() if price_tag else 'No Price'\n",
    "            \n",
    "            # EMI - Not provided directly on the page, can calculate or placeholder\n",
    "            emi = \"Not Available\"\n",
    "            \n",
    "            # Append details to lists\n",
    "            titles.append(title)\n",
    "            locations.append(location)\n",
    "            areas.append(area)\n",
    "            prices.append(price)\n",
    "            emis.append(emi)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {locality}\")\n",
    "\n",
    "# Scrape data for each locality\n",
    "for locality in localities:\n",
    "    scrape_locality_data(locality)\n",
    "\n",
    "# Create a DataFrame from the scraped data\n",
    "house_data = pd.DataFrame({\n",
    "    'Title': titles,\n",
    "    'Location': locations,\n",
    "    'Area': areas,\n",
    "    'Price': prices,\n",
    "    'EMI': emis\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(house_data)\n",
    "\n",
    "# Save data to CSV\n",
    "house_data.to_csv('nobroker_house_details.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14f0b4e6-f8a8-4076-aebb-a58153dd18f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Product Name     Price  \\\n",
      "0                   bewakoof x dc  No Price   \n",
      "1  bewakoof x house of the dragon  No Price   \n",
      "2          bewakoof x tom & jerry  No Price   \n",
      "3                       Bewakoof®  No Price   \n",
      "4                       Bewakoof®  No Price   \n",
      "5              bewakoof x peanuts  No Price   \n",
      "6                       Bewakoof®  No Price   \n",
      "7                       Bewakoof®  No Price   \n",
      "8          bewakoof x tom & jerry  No Price   \n",
      "9                       Bewakoof®  No Price   \n",
      "\n",
      "                                           Image URL  \n",
      "0  https://images.bewakoof.com/t640/men-s-black-a...  \n",
      "1  https://images.bewakoof.com/t640/men-s-black-h...  \n",
      "2  https://images.bewakoof.com/t640/women-s-blue-...  \n",
      "3  https://images.bewakoof.com/t640/women-aop-ove...  \n",
      "4  https://images.bewakoof.com/t640/men-s-black-w...  \n",
      "5  https://images.bewakoof.com/t640/women-s-orang...  \n",
      "6  https://images.bewakoof.com/t640/men-s-black-l...  \n",
      "7  https://images.bewakoof.com/t640/men-s-sun-kis...  \n",
      "8  https://images.bewakoof.com/t640/women-s-green...  \n",
      "9  https://images.bewakoof.com/t640/men-s-blue-ri...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the Bewakoof bestseller page\n",
    "url = \"https://www.bewakoof.com/bestseller?sort=popular\"\n",
    "\n",
    "# Send a request to fetch the content of the webpage\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the containers for the first 10 products\n",
    "    product_containers = soup.find_all('div', class_='productCardBox', limit=10)\n",
    "    \n",
    "    # Lists to store product details\n",
    "    product_names = []\n",
    "    product_prices = []\n",
    "    product_image_urls = []\n",
    "    \n",
    "    # Loop through each product container and extract details\n",
    "    for product in product_containers:\n",
    "        # Extract product name\n",
    "        name_tag = product.find('h3')\n",
    "        product_name = name_tag.text.strip() if name_tag else \"No Name\"\n",
    "        product_names.append(product_name)\n",
    "        \n",
    "        # Extract product price\n",
    "        price_tag = product.find('span', class_='discountedPriceText')\n",
    "        product_price = price_tag.text.strip() if price_tag else \"No Price\"\n",
    "        product_prices.append(product_price)\n",
    "        \n",
    "        # Extract product image URL\n",
    "        image_tag = product.find('img')\n",
    "        product_image_url = image_tag['src'] if image_tag else \"No Image URL\"\n",
    "        product_image_urls.append(product_image_url)\n",
    "    \n",
    "    # DataFrame to store the scraped data\n",
    "    products_df = pd.DataFrame({'Product Name': product_names,'Price': product_prices,'Image URL': product_image_urls})\n",
    "    \n",
    "    print(products_df)\n",
    "    \n",
    "    # Save the data to a CSV file if needed\n",
    "    # products_df.to_csv('bewakoof_bestsellers.csv', index=False)\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to retrieve the page.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f87bf28-8e18-47ba-b0e8-6801c8d8a50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Heading     Date  \\\n",
      "0   Stocks making the biggest moves midday: JetBlu...  No Date   \n",
      "1   JPMorgan top economist says the Fed should cut...  No Date   \n",
      "2   Dow falls more than 200 points as growth fears...  No Date   \n",
      "3   Friday's jobs report for August is going to be...  No Date   \n",
      "4   'Get Britain building again': New UK finance c...  No Date   \n",
      "5   Britain's Labour pulled off a thumping electio...  No Date   \n",
      "6   UK's Labour Party secures landslide victory in...  No Date   \n",
      "7   Labour does not have much headroom in terms of...  No Date   \n",
      "8   Outgoing UK PM Rishi Sunak to step down as Con...  No Date   \n",
      "9   Water shortages are likely brewing future wars...  No Date   \n",
      "10  Sweden's Volvo Cars scraps plan to sell only e...  No Date   \n",
      "11  ESG investors appear to be getting much more c...  No Date   \n",
      "12  Finland will soon bury spent nuclear fuel in t...  No Date   \n",
      "13  This water treatment startup is already a unic...  No Date   \n",
      "14  Passengers often ignore in-flight safety video...  No Date   \n",
      "15  Has Boeing shaken your confidence to fly? A ne...  No Date   \n",
      "16  Interest to visit Japan soars, while China str...  No Date   \n",
      "17  Can a robot give a decent massage? We tested o...  No Date   \n",
      "18  This ship travels the world full time — here's...  No Date   \n",
      "19  Bill Gates says this is the No. 1 unsolvable p...  No Date   \n",
      "20  Don't forget this resume section if you're loo...  No Date   \n",
      "21  One move can help your kid land a high credit ...  No Date   \n",
      "22  Why 'Blink Twice' star Haley Joel Osment trade...  No Date   \n",
      "23  How to master your money: Practical strategies...  No Date   \n",
      "24                     What are the economics of war?  No Date   \n",
      "25                    What is the internet of bodies?  No Date   \n",
      "26       How the world got into $315 trillion of debt  No Date   \n",
      "27  eVTOLS: Are flying cars finally becoming reality?  No Date   \n",
      "28                  How China's property bubble burst  No Date   \n",
      "\n",
      "                                                 Link  \n",
      "0   https://www.cnbc.com/2024/09/05/stocks-making-...  \n",
      "1   https://www.cnbc.com/2024/09/05/jpmorgan-top-e...  \n",
      "2   https://www.cnbc.com/2024/09/04/stock-market-t...  \n",
      "3   https://www.cnbc.com/2024/09/05/fridays-jobs-r...  \n",
      "4   https://www.cnbc.com/2024/07/08/uk-election-20...  \n",
      "5   https://www.cnbc.com/2024/07/05/uk-election-20...  \n",
      "6   https://www.cnbc.com/video/2024/07/05/uks-labo...  \n",
      "7   https://www.cnbc.com/video/2024/07/05/labour-d...  \n",
      "8   https://www.cnbc.com/2024/07/05/uk-election-ri...  \n",
      "9   https://www.cnbc.com/2024/09/05/water-wars-fla...  \n",
      "10  https://www.cnbc.com/2024/09/04/swedens-volvo-...  \n",
      "11  https://www.cnbc.com/2024/09/02/esg-sustainabl...  \n",
      "12  https://www.cnbc.com/2024/08/29/onkalo-finland...  \n",
      "13  https://www.cnbc.com/2024/08/27/this-water-tre...  \n",
      "14  https://www.cnbc.com/2024/09/05/youtube-video-...  \n",
      "15  https://www.cnbc.com/2024/09/02/fear-of-flying...  \n",
      "16  https://www.cnbc.com/2024/08/27/why-travel-dem...  \n",
      "17  https://www.cnbc.com/2024/08/22/a-robot-that-g...  \n",
      "18  https://www.cnbc.com/2024/08/21/ship-that-trav...  \n",
      "19  https://www.cnbc.com/2024/09/05/bill-gates-onl...  \n",
      "20  https://www.cnbc.com/2024/09/05/dont-forget-th...  \n",
      "21  https://www.cnbc.com/2024/09/05/pros-cons-of-a...  \n",
      "22  https://www.cnbc.com/2024/09/05/why-blink-twic...  \n",
      "23  https://www.cnbc.com/2024/07/10/achieve-financ...  \n",
      "24  https://www.cnbc.com/video/2024/08/07/what-are...  \n",
      "25  https://www.cnbc.com/video/2024/05/31/what-is-...  \n",
      "26  https://www.cnbc.com/video/2024/05/28/how-the-...  \n",
      "27  https://www.cnbc.com/video/2024/03/28/evtols-h...  \n",
      "28  https://www.cnbc.com/video/2024/02/29/how-chin...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "\n",
    "# Send a request to fetch the content of the webpage\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Lists to store scraped data\n",
    "    headings = []\n",
    "    dates = []\n",
    "    links = []\n",
    "    \n",
    "    # Find the news articles container\n",
    "    articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "    \n",
    "    for article in articles:\n",
    "        # Extract news heading\n",
    "        heading_tag = article.find('a')\n",
    "        heading = heading_tag.text.strip() if heading_tag else \"No Heading\"\n",
    "        headings.append(heading)\n",
    "        \n",
    "        # Extract news link\n",
    "        news_link = heading_tag['href'] if heading_tag else \"No Link\"\n",
    "        links.append(news_link)\n",
    "        \n",
    "        # Extract publication date if available (CNBC doesn't always show dates in this view)\n",
    "        date_tag = article.find_next('time')  # Find next time element\n",
    "        date = date_tag.text.strip() if date_tag else \"No Date\"\n",
    "        dates.append(date)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    news_df = pd.DataFrame({'Heading': headings,'Date': dates,'Link': links})\n",
    "    \n",
    "    print(news_df)\n",
    "    \n",
    "    # Save to CSV file\n",
    "    # news_df.to_csv('cnbc_world_news.csv', index=False)\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to retrieve the page.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d848a89-31bb-4795-aeb2-1450a9b5567a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve the page.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# URL of the KEAI Publishing Most Downloaded Articles page\n",
    "url = \"https://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/most-downloaded/articles/\"\n",
    "\n",
    "# Send a request to fetch the content of the webpage\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36'}\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Lists to store scraped data\n",
    "    titles = []\n",
    "    dates = []\n",
    "    authors = []\n",
    "    \n",
    "    # Find the containers for the articles\n",
    "    articles = soup.find_all('article', class_='article')\n",
    "    \n",
    "    for article in articles:\n",
    "        # Extract paper title\n",
    "        title_tag = article.find('h2', class_='title')\n",
    "        title = title_tag.text.strip() if title_tag else \"No Title\"\n",
    "        titles.append(title)\n",
    "        \n",
    "        # Extract publication date\n",
    "        date_tag = article.find('time')\n",
    "        date = date_tag.text.strip() if date_tag else \"No Date\"\n",
    "        dates.append(date)\n",
    "        \n",
    "        # Extract author names\n",
    "        author_tags = article.find_all('a', class_='author')\n",
    "        authors_list = [author.text.strip() for author in author_tags]\n",
    "        authors.append(\", \".join(authors_list) if authors_list else \"No Authors\")\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    articles_df = pd.DataFrame({'Paper Title': titles,'Date': dates,'Author': authors})\n",
    "    \n",
    "    # Display the DataFrame\n",
    "    print(articles_df)\n",
    "    \n",
    "    # Save to CSV file\n",
    "    # articles_df.to_csv('keaipublishing_most_downloaded_articles.csv', index=False)\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to retrieve the page.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
