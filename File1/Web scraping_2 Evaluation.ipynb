{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290f81b0-d4fa-4556-a5b1-609f72a2d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium pandas\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver_path = 'path_to_webdriver'  \n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "driver.get(\"https://www.naukri.com/\")\n",
    "\n",
    "search_box = driver.find_element('name', 'keyword')\n",
    "search_box.send_keys(\"Data Scientist\")\n",
    "\n",
    "search_button = driver.find_element('class name', 'search-btn')\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(3) \n",
    "\n",
    "location_filter = driver.find_element(\"xpath\", \"//label[@for='chk-Delhi / NCR-cityTypeGid-7']\")\n",
    "location_filter.click()\n",
    "\n",
    "time.sleep(2) \n",
    "\n",
    "salary_filter = driver.find_element(\"xpath\", \"//label[@for='chk-3-6 Lacs-salaryRange-12']\")\n",
    "salary_filter.click()\n",
    "\n",
    "time.sleep(3)  \n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "job_blocks = soup.find_all('article', class_='jobTuple bgWhite br4 mb-8')[:10]\n",
    "\n",
    "jobs = []\n",
    "for job in job_blocks:\n",
    "    job_title = job.find('a', class_='title fw500 ellipsis').text.strip()\n",
    "    job_location = job.find('li', class_='fleft grey-text br2 placeHolderLi location').text.strip()\n",
    "    company_name = job.find('a', class_='subTitle ellipsis fleft').text.strip()\n",
    "    experience_required = job.find('li', class_='fleft grey-text br2 placeHolderLi experience').text.strip()\n",
    "    \n",
    "    jobs.append([job_title, job_location, company_name, experience_required])\n",
    "\n",
    "df = pd.DataFrame(jobs, columns=['Job Title', 'Job Location', 'Company Name', 'Experience Required'])\n",
    "print(df)\n",
    "\n",
    "# CSV\n",
    "df.to_csv('data_scientist_jobs_delhi.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1021e96c-dc43-4499-89b6-202cf6baa175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver_path = 'path_to_webdriver'  \n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "\n",
    "job_title_field = driver.find_element('id', 'id_q')  # Locate job title input field\n",
    "job_title_field.send_keys(\"Data Scientist\")\n",
    "\n",
    "location_field = driver.find_element('id', 'id_loc')  # Locate location input field\n",
    "location_field.send_keys(\"Bangalore\")\n",
    "\n",
    "search_button = driver.find_element('xpath', \"//button[@class='btn btn-primary search-btn']\")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "job_blocks = soup.find_all('li', class_='search_listing')[:10]  # Select the first 10 job listings\n",
    "\n",
    "jobs = []\n",
    "for job in job_blocks:\n",
    "   \n",
    "    job_title = job.find('h2').text.strip()\n",
    "\n",
    "    job_location = job.find('em', class_='loc').text.strip()\n",
    "\n",
    "    company_name = job.find('span', class_='org').text.strip()\n",
    "\n",
    "    experience = job.find('li', class_='exp').text.strip()\n",
    "\n",
    "    jobs.append([job_title, job_location, company_name, experience])\n",
    "\n",
    "\n",
    "df = pd.DataFrame(jobs, columns=['Job Title', 'Job Location', 'Company Name', 'Experience Required'])\n",
    "print(df)\n",
    "\n",
    "# CSV file\n",
    "df.to_csv('data_scientist_jobs_bangalore.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07ddeae7-7dd6-4b19-986f-f8d6a58c906d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Rating, Review Summary, Full Review]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()  \n",
    "\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "def scroll_down():\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "\n",
    "# Variables \n",
    "ratings = []\n",
    "review_summaries = []\n",
    "full_reviews = []\n",
    "\n",
    "# Loop until 100 reviews\n",
    "while len(ratings) < 100:\n",
    "   \n",
    "    time.sleep(2)\n",
    "    \n",
    "    rating_elements = driver.find_elements(By.CSS_SELECTOR, \"div._3LWZlK._1BLPMq\")\n",
    "    for rating in rating_elements:\n",
    "        ratings.append(rating.text)\n",
    "    \n",
    "    summary_elements = driver.find_elements(By.CSS_SELECTOR, \"p._2-N8zT\")\n",
    "    for summary in summary_elements:\n",
    "        review_summaries.append(summary.text)\n",
    "    \n",
    "    full_review_elements = driver.find_elements(By.CSS_SELECTOR, \"div.t-ZTKy div\")\n",
    "    for review in full_review_elements:\n",
    "        full_reviews.append(review.text)\n",
    "    \n",
    "    if len(ratings) >= 100:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        next_button = driver.find_element(By.XPATH, \"//a[@class='_1LKTO3'][2]\")\n",
    "        next_button.click()\n",
    "    except:\n",
    "        break  \n",
    "        \n",
    "    scroll_down()\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n",
    "\n",
    "# Limit the data to 100 reviews\n",
    "ratings = ratings[:100]\n",
    "review_summaries = review_summaries[:100]\n",
    "full_reviews = full_reviews[:100]\n",
    "\n",
    "# DataFrame\n",
    "reviews_data = pd.DataFrame({\"Rating\": ratings,\"Review Summary\": review_summaries,\"Full Review\": full_reviews})\n",
    "\n",
    "print(reviews_data)\n",
    "\n",
    "# CSV file\n",
    "reviews_data.to_csv(\"iphone_11_reviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a88e9c0-ab2f-4c0a-8a56-5e0835a4e418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver_path = 'path_to_webdriver'\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "driver.get(\"https://www.flipkart.com/\")\n",
    "\n",
    "try:\n",
    "    close_popup = driver.find_element(\"xpath\", \"//button[contains(text(),'âœ•')]\")\n",
    "    close_popup.click()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "search_box = driver.find_element(\"name\", \"q\")\n",
    "search_box.send_keys(\"sneakers\")\n",
    "search_button = driver.find_element(\"xpath\", \"//button[@type='submit']\")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "sneakers = []\n",
    "while len(sneakers) < 100:\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    product_cards = soup.find_all('div', class_='_1AtVbE') \n",
    "    \n",
    "    for product in product_cards:\n",
    "    \n",
    "        try:\n",
    "            brand = product.find('div', class_='_2WkVRV').text.strip()\n",
    "            description = product.find('a', class_='IRpwTa').text.strip()\n",
    "            price = product.find('div', class_='_30jeq3').text.strip()\n",
    "            \n",
    "            sneakers.append([brand, description, price])\n",
    "            \n",
    "            if len(sneakers) >= 100:\n",
    "                break\n",
    "        except:\n",
    "            continue \n",
    "            \n",
    "    if len(sneakers) < 100:\n",
    "        try:\n",
    "            next_button = driver.find_element(\"xpath\", \"//a[@class='_1LKTO3'][2]\")\n",
    "            next_button.click()\n",
    "            time.sleep(3)  \n",
    "        except:\n",
    "            break  \n",
    "            \n",
    "df = pd.DataFrame(sneakers[:100], columns=['Brand', 'Product Description', 'Price'])\n",
    "print(df)\n",
    "\n",
    "# CSV file\n",
    "df.to_csv('sneakers_flipkart.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a32646d6-3d86-4ff3-a906-b79d689d595a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title Ratings     Price\n",
      "0  HP Laptop 15s, 12th Gen Intel Core i7-1255U, 1...            59,990\n",
      "1  Dell [Smartchoice] Inspiron 5430 Thin & Light ...            75,490\n",
      "2  Acer ALG 13th Gen Intel Core i7 Gaming Laptop ...            71,990\n",
      "3  (Refurbished) Dell Latitude 7480 14in FHD Lapt...            27,531\n",
      "4  Acer Aspire Lite 12th Gen Intel Core i7-1255U ...            49,990\n",
      "5  Dell Inspiron 3530 Laptop, 13th Generation Int...            69,890\n",
      "6  Lenovo IdeaPad Slim 3 Intel Core i7 12th Gen 1...            59,000\n",
      "7  Acer Aspire 3 Intel Core i7 12th Gen 1255U - (...            57,990\n",
      "8  ASUS TUF Gaming F15, 15.6\" (39.62cm) FHD 144Hz...          1,08,048\n",
      "9  MSI Thin 15, Intel 13th Gen. Core i7-13620H, 4...            69,990\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "search_box = driver.find_element(By.ID, \"twotabsearchtextbox\")\n",
    "search_box.send_keys(\"Laptop\")\n",
    "\n",
    "search_button = driver.find_element(By.ID, \"nav-search-submit-button\")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "driver.execute_script(\"window.scrollBy(0, 200);\")\n",
    "time.sleep(2)\n",
    "\n",
    "cpu_filter = driver.find_element(By.XPATH, \"//li[@aria-label='Intel Core i7']//i\")\n",
    "cpu_filter.click()\n",
    "\n",
    "time.sleep(5)\n",
    "\n",
    "titles = []\n",
    "ratings = []\n",
    "prices = []\n",
    "\n",
    "while len(titles) < 10:\n",
    "    \n",
    "    title_elements = driver.find_elements(By.XPATH, \"//span[@class='a-size-medium a-color-base a-text-normal']\")\n",
    "    for title in title_elements:\n",
    "        if len(titles) < 10:\n",
    "            titles.append(title.text)\n",
    "    \n",
    "    rating_elements = driver.find_elements(By.XPATH, \"//span[@class='a-icon-alt']\")\n",
    "    for rating in rating_elements:\n",
    "        if len(ratings) < 10:\n",
    "            ratings.append(rating.text)\n",
    "    \n",
    "    price_elements = driver.find_elements(By.XPATH, \"//span[@class='a-price-whole']\")\n",
    "    for price in price_elements:\n",
    "        if len(prices) < 10:\n",
    "            prices.append(price.text)\n",
    "    \n",
    "    if len(titles) >= 10:\n",
    "        break\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# Close the WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# DataFrame\n",
    "laptop_data = pd.DataFrame({\"Title\": titles[:10], \"Ratings\": ratings[:10], \"Price\": prices[:10]})\n",
    "\n",
    "# DataFrame\n",
    "print(laptop_data)\n",
    "\n",
    "# CSV file\n",
    "laptop_data.to_csv(\"laptops_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887b047-3100-49f7-bfa3-7b2754d90c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver_path = 'path_to_webdriver' \n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "\n",
    "time.sleep(2) \n",
    "top_quotes_button = driver.find_element(\"xpath\", \"//a[text()='Top Quotes']\")\n",
    "top_quotes_button.click()\n",
    "\n",
    "quotes_data = []\n",
    "\n",
    "while len(quotes_data) < 1000:\n",
    "    # Parse the page source with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    \n",
    "    quote_blocks = soup.find_all('div', class_='wrap-block')\n",
    "\n",
    "    for block in quote_blocks:\n",
    "        try:\n",
    "        \n",
    "            quote_text = block.find('a', class_='title').text.strip()\n",
    "            \n",
    "            author = block.find('div', class_='author').text.strip()\n",
    "\n",
    "            quote_type = block.find('div', class_='tags').text.strip()\n",
    "\n",
    "            quotes_data.append([quote_text, author, quote_type])\n",
    "        \n",
    "            if len(quotes_data) >= 1000:\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if len(quotes_data) < 1000:\n",
    "        try:\n",
    "            next_button = driver.find_element(\"xpath\", \"//li[@class='next']/a\")\n",
    "            next_button.click()\n",
    "            time.sleep(3)  \n",
    "        except:\n",
    "            break  \n",
    "\n",
    "df = pd.DataFrame(quotes_data[:1000], columns=['Quote', 'Author', 'Type Of Quote'])\n",
    "print(df)\n",
    "\n",
    "# CSV file\n",
    "df.to_csv('top_1000_quotes.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f8ea3f6-b4bf-41c1-8d7a-da499d98f057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Name, Born-Dead, Term of Office, Remarks]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.jagranjosh.com/general-knowledge/list-of-all-prime-ministers-of-india-1473165149-1\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "table = soup.find('table') \n",
    "\n",
    "# Variables \n",
    "names = []\n",
    "born_dead = []\n",
    "terms = []\n",
    "remarks = []\n",
    "\n",
    "# Loop\n",
    "for row in table.find_all('tr')[1:]:  \n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) == 4:  \n",
    "        names.append(cells[0].text.strip())           # Prime Minister's Name\n",
    "        born_dead.append(cells[1].text.strip())       # Born-Dead\n",
    "        terms.append(cells[2].text.strip())           # Term of Office\n",
    "        remarks.append(cells[3].text.strip())         # Remarks\n",
    "\n",
    "pm_data = pd.DataFrame({\"Name\": names,\"Born-Dead\": born_dead,\"Term of Office\": terms, \"Remarks\": remarks})\n",
    "\n",
    "# DataFrame\n",
    "print(pm_data)\n",
    "\n",
    "# CSV file\n",
    "pm_data.to_csv(\"former_prime_ministers_of_india.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5130fc-bb22-46bc-8585-9017fcd2c46a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "driver_path = 'path_to_webdriver'\n",
    "driver = webdriver.Chrome(executable_path=driver_path)\n",
    "\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "\n",
    "search_box = driver.find_element(\"name\", \"query\")\n",
    "search_box.send_keys(\"50 most expensive cars\")\n",
    "\n",
    "search_button = driver.find_element(\"xpath\", \"//button[@type='submit']\")\n",
    "search_button.click()\n",
    "\n",
    "time.sleep(3)\n",
    "articles = driver.find_elements(\"xpath\", \"//a[contains(@href, '50-most-expensive-cars')]\")\n",
    "\n",
    "if articles:\n",
    "    articles[0].click() \n",
    "else:\n",
    "    print(\"Could not find the relevant article.\")\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "time.sleep(5)  \n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "car_list = []\n",
    "car_elements = soup.find_all('div', class_='list-item')  \n",
    "for car in car_elements:\n",
    "    try:\n",
    "    \n",
    "        car_name = car.find('h3').text.strip()\n",
    "\n",
    "        car_price = car.find('p').text.strip()\n",
    "\n",
    "        car_list.append([car_name, car_price])\n",
    "   \n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame(car_list, columns=['Car Name', 'Price'])\n",
    "\n",
    "print(df.head(50))\n",
    "\n",
    "# CSV file\n",
    "df.to_csv('50_most_expensive_cars.csv', index=False)\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
